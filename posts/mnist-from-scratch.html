<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>I Taught a Computer to Read Handwriting Using Only Math</title>
  <link rel="stylesheet" href="../css/base.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:ital,wght@0,400;0,700;0,900;1,400;1,700&family=Lora:ital,wght@0,400;0,500;1,400&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

  <style>
    :root {
      --bg:        #faf8f3;
      --surface:   #f2ede3;
      --ink:       #1a1814;
      --ink-muted: #5a5650;
      --accent:    #c4622d;
      --accent-lt: #f5e6dc;
      --code-bg:   #1e1c18;
      --code-fg:   #e8dfc8;
      --border:    #ddd8ce;
      --green:     #2d7a4f;
      --blue:      #1d5fa8;
      --purple:    #6b3fa0;
    }

    *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }

    html { scroll-behavior: smooth; }

    body {
      background: var(--bg);
      color: var(--ink);
      font-family: 'Lora', Georgia, serif;
      font-size: 19px;
      line-height: 1.8;
      -webkit-font-smoothing: antialiased;
    }

    /* â”€â”€â”€ PROGRESS BAR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    #progress-bar {
      position: fixed; top: 0; left: 0;
      height: 3px; width: 0%;
      background: var(--accent);
      z-index: 1000;
      transition: width 0.1s linear;
    }

    /* â”€â”€â”€ HEADER / HERO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    header {
      background: var(--ink);
      color: #faf8f3;
      padding: 100px 24px 80px;
      text-align: center;
      position: relative;
      overflow: hidden;
      margin-top: var(--nav-h, 80px);
    }

    header::before {
      content: '';
      position: absolute; inset: 0;
      background: radial-gradient(ellipse at 70% 30%, rgba(196,98,45,0.18) 0%, transparent 60%),
                  radial-gradient(ellipse at 20% 80%, rgba(29,95,168,0.12) 0%, transparent 50%);
      pointer-events: none;
    }

    .hero-tag {
      display: inline-block;
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      letter-spacing: 0.15em;
      text-transform: uppercase;
      color: var(--accent);
      border: 1px solid var(--accent);
      padding: 5px 14px;
      border-radius: 2px;
      margin-bottom: 32px;
      opacity: 0;
      animation: fadeUp 0.6s 0.1s forwards;
    }

    h1 {
      font-family: 'Playfair Display', serif;
      font-size: clamp(32px, 6vw, 62px);
      font-weight: 900;
      line-height: 1.1;
      max-width: 820px;
      margin: 0 auto 28px;
      color: #faf8f3;
      opacity: 0;
      animation: fadeUp 0.7s 0.2s forwards;
    }

    h1 em {
      color: var(--accent);
      font-style: italic;
    }

    .subtitle {
      font-family: 'Lora', serif;
      font-size: 19px;
      color: #c8c2b6;
      max-width: 580px;
      margin: 0 auto 48px;
      font-style: italic;
      opacity: 0;
      animation: fadeUp 0.7s 0.3s forwards;
    }

    .meta {
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 24px;
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      color: #8a857c;
      letter-spacing: 0.05em;
      opacity: 0;
      animation: fadeUp 0.7s 0.4s forwards;
    }

    .meta-dot { color: var(--accent); }

    @keyframes fadeUp {
      from { opacity: 0; transform: translateY(20px); }
      to   { opacity: 1; transform: translateY(0); }
    }

    /* â”€â”€â”€ MAIN LAYOUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .container {
      max-width: 740px;
      margin: 0 auto;
      padding: 0 24px;
    }

    article {
      padding: 72px 0 100px;
    }

    /* â”€â”€â”€ TYPOGRAPHY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    h2 {
      font-family: 'Playfair Display', serif;
      font-size: clamp(24px, 4vw, 34px);
      font-weight: 700;
      color: var(--ink);
      margin: 72px 0 20px;
      line-height: 1.2;
      padding-bottom: 16px;
      border-bottom: 2px solid var(--border);
    }

    h2 .step-num {
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
      font-weight: 400;
      color: var(--accent);
      display: block;
      margin-bottom: 6px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
    }

    h3 {
      font-family: 'Playfair Display', serif;
      font-size: 22px;
      font-weight: 700;
      margin: 40px 0 12px;
      color: var(--ink);
    }

    p { margin-bottom: 24px; }

    strong { font-weight: 700; color: var(--ink); }

    a { color: var(--accent); text-decoration: underline; text-underline-offset: 3px; }

    /* â”€â”€â”€ PULL QUOTE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    blockquote {
      border-left: 4px solid var(--accent);
      margin: 40px 0;
      padding: 20px 32px;
      background: var(--accent-lt);
      border-radius: 0 6px 6px 0;
    }

    blockquote p {
      font-family: 'Playfair Display', serif;
      font-size: 22px;
      font-style: italic;
      color: var(--ink);
      margin: 0;
      line-height: 1.5;
    }

    /* â”€â”€â”€ CODE BLOCKS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .code-block {
      margin: 32px 0;
      border-radius: 8px;
      overflow: hidden;
      box-shadow: 0 4px 24px rgba(0,0,0,0.15);
    }

    .code-header {
      background: #2a2720;
      padding: 10px 20px;
      display: flex;
      align-items: center;
      gap: 12px;
    }

    .code-dots { display: flex; gap: 6px; }

    .code-dot {
      width: 12px; height: 12px;
      border-radius: 50%;
    }

    .dot-r { background: #ff5f56; }
    .dot-y { background: #ffbd2e; }
    .dot-g { background: #27c93f; }

    .code-label {
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      color: #7a746a;
      margin-left: auto;
      letter-spacing: 0.05em;
    }

    pre {
      background: var(--code-bg);
      color: var(--code-fg);
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      line-height: 1.7;
      padding: 24px 28px;
      overflow-x: auto;
      margin: 0;
    }

    .kw  { color: #e06c75; }
    .fn  { color: #61afef; }
    .str { color: #98c379; }
    .num { color: #d19a66; }
    .cm  { color: #7d7563; font-style: italic; }
    .op  { color: #c678dd; }

    /* â”€â”€â”€ CALLOUT BOXES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .callout {
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 20px 24px;
      margin: 32px 0;
      background: var(--surface);
      display: flex;
      gap: 16px;
      align-items: flex-start;
    }

    .callout-icon {
      font-size: 22px;
      flex-shrink: 0;
      margin-top: 2px;
    }

    .callout p { margin: 0; font-size: 16px; color: var(--ink-muted); line-height: 1.6; }

    .callout-title {
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      font-weight: 500;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      margin-bottom: 6px;
      color: var(--ink);
    }

    /* â”€â”€â”€ VISUAL DIAGRAMS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .diagram-wrap {
      margin: 40px 0;
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 36px 24px;
      text-align: center;
    }

    .diagram-title {
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      text-transform: uppercase;
      letter-spacing: 0.12em;
      color: var(--ink-muted);
      margin-bottom: 32px;
    }

    .network-svg-wrap svg {
      width: 100%;
      max-width: 680px;
      height: auto;
    }

    /* Training Loop Diagram */
    .loop-diagram {
      display: flex;
      align-items: center;
      justify-content: center;
      flex-wrap: wrap;
      gap: 0;
      margin: 32px 0;
    }

    .loop-step {
      background: white;
      border: 2px solid var(--border);
      border-radius: 8px;
      padding: 14px 20px;
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
      text-align: center;
      min-width: 110px;
      position: relative;
      transition: border-color 0.2s, transform 0.2s;
    }

    .loop-step:hover {
      border-color: var(--accent);
      transform: translateY(-2px);
    }

    .loop-step .ls-icon { font-size: 24px; display: block; margin-bottom: 6px; }
    .loop-step .ls-label { color: var(--ink-muted); font-size: 11px; display: block; margin-top: 4px; }
    .loop-step .ls-name { color: var(--ink); font-weight: 500; }

    .loop-arrow {
      font-size: 20px;
      color: var(--accent);
      padding: 0 6px;
      font-weight: bold;
    }

    /* Math formula box */
    .formula-box {
      background: var(--code-bg);
      color: var(--code-fg);
      border-radius: 8px;
      padding: 24px 28px;
      margin: 28px 0;
      font-family: 'JetBrains Mono', monospace;
      font-size: 15px;
      line-height: 2;
      overflow-x: auto;
    }

    .formula-row {
      display: flex;
      align-items: center;
      gap: 16px;
      margin-bottom: 8px;
    }

    .formula-label {
      color: #7d7563;
      font-size: 12px;
      min-width: 100px;
      text-align: right;
      font-style: italic;
    }

    .formula-eq { color: #c678dd; margin: 0 8px; }
    .formula-val { color: #98c379; }
    .formula-comment { color: #5c5548; font-size: 12px; margin-left: 12px; }

    /* Pixel grid visual */
    .mnist-visual {
      display: flex;
      align-items: center;
      gap: 32px;
      flex-wrap: wrap;
      justify-content: center;
      margin: 32px 0;
    }

    .pixel-grid {
      display: grid;
      grid-template-columns: repeat(14, 1fr);
      gap: 2px;
      width: 168px;
    }

    .pixel {
      width: 10px;
      height: 10px;
      border-radius: 1px;
    }

    .mnist-label-arrow {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 8px;
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
      color: var(--ink-muted);
    }

    .mnist-label-arrow .arrow { font-size: 28px; color: var(--accent); }

    .output-probs {
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
    }

    .prob-row {
      display: flex;
      align-items: center;
      gap: 8px;
      margin-bottom: 4px;
    }

    .prob-digit { width: 14px; color: var(--ink-muted); }

    .prob-bar-wrap {
      width: 120px;
      height: 12px;
      background: var(--border);
      border-radius: 2px;
      overflow: hidden;
    }

    .prob-bar {
      height: 100%;
      border-radius: 2px;
      background: var(--blue);
      transition: width 1s ease;
    }

    .prob-bar.top { background: var(--accent); }
    .prob-val { color: var(--ink-muted); font-size: 11px; }

    /* Activation function charts */
    .activation-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
      gap: 20px;
      margin: 32px 0;
    }

    .activation-card {
      background: white;
      border: 1px solid var(--border);
      border-radius: 10px;
      padding: 20px;
      text-align: center;
    }

    .activation-card h4 {
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      font-weight: 500;
      color: var(--ink);
      margin-bottom: 4px;
    }

    .activation-card .ac-desc {
      font-size: 12px;
      color: var(--ink-muted);
      margin-bottom: 16px;
      font-family: 'JetBrains Mono', monospace;
    }

    .activation-card canvas {
      display: block;
      margin: 0 auto;
    }

    /* Results table */
    .results-table {
      width: 100%;
      border-collapse: collapse;
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      margin: 32px 0;
    }

    .results-table th {
      background: var(--ink);
      color: #faf8f3;
      padding: 12px 16px;
      text-align: left;
      font-size: 11px;
      text-transform: uppercase;
      letter-spacing: 0.1em;
    }

    .results-table td {
      padding: 12px 16px;
      border-bottom: 1px solid var(--border);
      color: var(--ink-muted);
    }

    .results-table tr:nth-child(even) td { background: var(--surface); }
    .results-table td:last-child { color: var(--green); font-weight: 500; }

    /* TOC */
    .toc {
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 10px;
      padding: 28px 32px;
      margin: 48px 0;
    }

    .toc-title {
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      text-transform: uppercase;
      letter-spacing: 0.12em;
      color: var(--ink-muted);
      margin-bottom: 16px;
    }

    .toc ol {
      padding-left: 20px;
      counter-reset: toc-counter;
      list-style: none;
    }

    .toc li {
      counter-increment: toc-counter;
      margin-bottom: 8px;
    }

    .toc li::before {
      content: counter(toc-counter, decimal-leading-zero) ". ";
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      color: var(--accent);
      margin-right: 4px;
    }

    .toc a {
      color: var(--ink);
      text-decoration: none;
      font-size: 16px;
      font-family: 'Lora', serif;
      transition: color 0.2s;
    }

    .toc a:hover { color: var(--accent); }

    /* Section divider */
    .divider {
      display: flex;
      align-items: center;
      gap: 16px;
      margin: 60px 0;
      color: var(--border);
    }

    .divider::before, .divider::after {
      content: '';
      flex: 1;
      height: 1px;
      background: var(--border);
    }

    .divider span {
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      color: var(--accent);
      letter-spacing: 0.1em;
    }

    /* Footer */
    footer {
      background: var(--ink);
      color: #8a857c;
      padding: 48px 24px;
      text-align: center;
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
    }

    footer a { color: var(--accent); }

    /* Inline code */
    code {
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      background: var(--surface);
      border: 1px solid var(--border);
      padding: 2px 7px;
      border-radius: 4px;
      color: var(--accent);
    }

    /* â”€â”€â”€ SCROLL ANIMATIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .reveal {
      opacity: 0;
      transform: translateY(28px);
      transition: opacity 0.7s ease, transform 0.7s ease;
    }

    .reveal.visible {
      opacity: 1;
      transform: translateY(0);
    }

    /* â”€â”€â”€ RESPONSIVE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    @media (max-width: 600px) {
      body { font-size: 17px; }
      .loop-diagram { gap: 8px; }
      .loop-arrow { display: none; }
      .formula-row { flex-direction: column; align-items: flex-start; gap: 4px; }
      .formula-label { text-align: left; }
    }
  </style>
</head>
<body>

<canvas id="starfield"></canvas>

<!-- Portfolio Navigation -->
<nav class="nav">
  <a href="../index.html" class="nav__logo">
    <span class="nav__logo-name">Zannatul Naim</span>
    <span class="nav__logo-sub">AI Researcher</span>
  </a>
  <ul class="nav__links">
    <li><a href="../index.html">Home</a></li>
    <li><a href="../about.html">About</a></li>
    <li><a href="../research.html">Research</a></li>
    <li><a href="../skills.html">Skills</a></li>
    <li><a href="../blog.html">Blog</a></li>
    <li><a href="../contact.html">Contact</a></li>
  </ul>
  <button class="nav__hamburger" aria-label="Toggle menu">
    <span></span><span></span><span></span>
  </button>
</nav>
<div class="nav__mobile">
  <a href="../index.html">Home</a>
  <a href="../about.html">About</a>
  <a href="../research.html">Research</a>
  <a href="../skills.html">Skills</a>
  <a href="../blog.html">Blog</a>
  <a href="../contact.html">Contact</a>
</div>

<div id="progress-bar"></div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• HERO â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<header>
  <div class="hero-tag">Deep Learning From Scratch Â· Tutorial</div>
  <h1>I Taught a Computer to <em>Read Handwriting</em><br>Using Only Math</h1>
  <p class="subtitle">A step-by-step guide to building a neural network from absolute zero â€” no TensorFlow, no PyTorch, just NumPy and your own two hands.</p>
  <div class="meta">
    <span>15 min read</span>
    <span class="meta-dot">â—†</span>
    <span>Beginner Friendly</span>
    <span class="meta-dot">â—†</span>
    <span>97%+ Accuracy</span>
    <span class="meta-dot">â—†</span>
    <span>Full Code Included</span>
  </div>
</header>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• ARTICLE â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<div class="container">
<article>

  <!-- TOC -->
  <div class="toc reveal">
    <div class="toc-title">Table of Contents</div>
    <ol>
      <li><a href="#intro">The Big Picture: What Are We Building?</a></li>
      <li><a href="#data">The Dataset: MNIST</a></li>
      <li><a href="#architecture">Network Architecture</a></li>
      <li><a href="#forward">Forward Pass: Making a Prediction</a></li>
      <li><a href="#loss">Loss Function: Measuring How Wrong We Are</a></li>
      <li><a href="#backprop">Backpropagation: Learning from Mistakes</a></li>
      <li><a href="#training">Training Loop: Putting It All Together</a></li>
      <li><a href="#results">Results & What to Try Next</a></li>
    </ol>
  </div>

  <!-- â•â•â•â• INTRO â•â•â•â• -->
  <section id="intro">
    <h2><span class="step-num">Prologue</span>The Big Picture</h2>

    <p class="reveal">When I first heard the phrase "neural network," I pictured some kind of mystical black box â€” something only PhDs at Google could understand. Then I actually looked inside one. What I found was refreshingly simple: it's just a chain of matrix multiplications, a few clever math tricks, and one elegant feedback loop called backpropagation.</p>

    <p class="reveal">By the end of this article, you'll build a neural network that can read handwritten digits with <strong>97%+ accuracy</strong> â€” from scratch, using nothing but Python and NumPy. No magic frameworks. No abstraction layers hiding what's really happening.</p>

    <blockquote class="reveal">
      <p>"To truly understand deep learning, you have to build it yourself at least once. Every abstraction makes sense once you've written the math by hand."</p>
    </blockquote>

    <p class="reveal">Here's the training loop we're building â€” this single cycle is the heartbeat of every neural network ever trained:</p>

    <!-- Training loop diagram -->
    <div class="diagram-wrap reveal">
      <div class="diagram-title">The Neural Network Training Cycle</div>
      <div class="loop-diagram">
        <div class="loop-step">
          <span class="ls-icon">ğŸ“·</span>
          <span class="ls-name">Input</span>
          <span class="ls-label">raw pixels</span>
        </div>
        <span class="loop-arrow">â†’</span>
        <div class="loop-step">
          <span class="ls-icon">âš¡</span>
          <span class="ls-name">Forward</span>
          <span class="ls-label">predict</span>
        </div>
        <span class="loop-arrow">â†’</span>
        <div class="loop-step">
          <span class="ls-icon">ğŸ“‰</span>
          <span class="ls-name">Loss</span>
          <span class="ls-label">measure error</span>
        </div>
        <span class="loop-arrow">â†’</span>
        <div class="loop-step">
          <span class="ls-icon">ğŸ”„</span>
          <span class="ls-name">Backward</span>
          <span class="ls-label">blame weights</span>
        </div>
        <span class="loop-arrow">â†’</span>
        <div class="loop-step">
          <span class="ls-icon">ğŸ¯</span>
          <span class="ls-name">Update</span>
          <span class="ls-label">improve</span>
        </div>
        <span class="loop-arrow">â†©</span>
      </div>
      <p style="font-family:JetBrains Mono,monospace;font-size:12px;color:var(--ink-muted);margin-top:16px;">Repeat ~500,000 times and you get a model that reads handwriting.</p>
    </div>
  </section>

  <div class="divider"><span>Â§</span></div>

  <!-- â•â•â•â• DATASET â•â•â•â• -->
  <section id="data">
    <h2><span class="step-num">Step 01</span>The Dataset: MNIST</h2>

    <p class="reveal">MNIST (Modified National Institute of Standards and Technology) is the "Hello, World" of machine learning. It's a collection of <strong>70,000 handwritten digits</strong> â€” 60,000 for training, 10,000 for testing â€” each one a 28Ã—28 grayscale image.</p>

    <p class="reveal">Every image is just a grid of pixel values between 0 (black) and 255 (white). We flatten that 28Ã—28 grid into a single row of 784 numbers. That row is what we feed into our network.</p>

    <!-- Pixel visual -->
    <div class="diagram-wrap reveal">
      <div class="diagram-title">How MNIST Images Are Stored</div>
      <div class="mnist-visual">
        <div>
          <div class="pixel-grid" id="pixel-grid-display"></div>
          <p style="font-family:JetBrains Mono,monospace;font-size:11px;color:var(--ink-muted);margin-top:8px;text-align:center;">28Ã—28 pixels = 784 values</p>
        </div>
        <div class="mnist-label-arrow">
          <span style="font-size:12px;">flatten</span>
          <span class="arrow">â†’</span>
          <span style="font-size:12px;">[0, 0, 12, 201, 255, ...]</span>
          <span style="font-size:10px;margin-top:4px;">784 numbers</span>
        </div>
        <div>
          <div class="output-probs" id="probs-display"></div>
          <p style="font-family:JetBrains Mono,monospace;font-size:11px;color:var(--ink-muted);margin-top:8px;text-align:center;">10 output probabilities</p>
        </div>
      </div>
    </div>

    <p class="reveal">We need one more preprocessing step: <strong>one-hot encoding</strong> the labels. Instead of storing the label "3" as the number 3, we store it as a vector of ten zeros with a single 1 in position 3:</p>

    <div class="formula-box reveal">
      <div class="formula-row">
        <span class="formula-label">label "3"</span>
        <span class="formula-eq">â†’</span>
        <span class="formula-val">[ 0, 0, 0, 1, 0, 0, 0, 0, 0, 0 ]</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">label "7"</span>
        <span class="formula-eq">â†’</span>
        <span class="formula-val">[ 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 ]</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">label "0"</span>
        <span class="formula-eq">â†’</span>
        <span class="formula-val">[ 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]</span>
      </div>
    </div>

    <p class="reveal">Why? Because our network outputs a vector of 10 probabilities, and we need the true answer to be in the same shape to compute the error. It also lets us process the entire batch at once using matrix operations â€” no slow Python loops.</p>

    <div class="code-block reveal">
      <div class="code-header">
        <div class="code-dots"><div class="code-dot dot-r"></div><div class="code-dot dot-y"></div><div class="code-dot dot-g"></div></div>
        <div class="code-label">preprocessing.py</div>
      </div>
      <pre><span class="cm"># Normalize: pixel values 0â€“255  â†’  0.0â€“1.0</span>
<span class="cm"># Smaller numbers = more stable training</span>
X_train = X_train / <span class="num">255.0</span>
X_test  = X_test  / <span class="num">255.0</span>

<span class="cm"># One-hot encode labels</span>
<span class="kw">def</span> <span class="fn">one_hot</span>(labels, num_classes=<span class="num">10</span>):
    encoded = np.zeros((len(labels), num_classes))
    encoded[np.arange(len(labels)), labels] = <span class="num">1.0</span>
    <span class="kw">return</span> encoded

Y_train = one_hot(y_train)  <span class="cm"># shape: (60000, 10)</span>
Y_test  = one_hot(y_test)   <span class="cm"># shape: (10000, 10)</span></pre>
    </div>
  </section>

  <div class="divider"><span>Â§</span></div>

  <!-- â•â•â•â• ARCHITECTURE â•â•â•â• -->
  <section id="architecture">
    <h2><span class="step-num">Step 02</span>Network Architecture</h2>

    <p class="reveal">Our network has three layers: an input layer, one hidden layer, and an output layer. This is called a <strong>shallow network</strong> â€” and it's surprisingly powerful.</p>

    <!-- Network SVG Diagram -->
    <div class="diagram-wrap reveal">
      <div class="diagram-title">Network Architecture Â· 784 â†’ 128 â†’ 10</div>
      <div class="network-svg-wrap">
        <svg viewBox="0 0 680 260" xmlns="http://www.w3.org/2000/svg" style="font-family:'JetBrains Mono',monospace;">
          <rect width="680" height="260" fill="#f2ede3" rx="8"/>
          <text x="90"  y="24" text-anchor="middle" font-size="11" fill="#5a5650" letter-spacing="1">INPUT</text>
          <text x="340" y="24" text-anchor="middle" font-size="11" fill="#5a5650" letter-spacing="1">HIDDEN</text>
          <text x="590" y="24" text-anchor="middle" font-size="11" fill="#5a5650" letter-spacing="1">OUTPUT</text>
          <text x="90"  y="248" text-anchor="middle" font-size="10" fill="#c4622d">784 neurons</text>
          <text x="340" y="248" text-anchor="middle" font-size="10" fill="#c4622d">128 neurons</text>
          <text x="590" y="248" text-anchor="middle" font-size="10" fill="#c4622d">10 neurons</text>
          <g stroke="#ddd8ce" stroke-width="0.6" opacity="0.8">
            <line x1="120" y1="60"  x2="310" y2="60"/>
            <line x1="120" y1="60"  x2="310" y2="90"/>
            <line x1="120" y1="60"  x2="310" y2="120"/>
            <line x1="120" y1="90"  x2="310" y2="60"/>
            <line x1="120" y1="90"  x2="310" y2="90"/>
            <line x1="120" y1="90"  x2="310" y2="120"/>
            <line x1="120" y1="120" x2="310" y2="60"/>
            <line x1="120" y1="120" x2="310" y2="90"/>
            <line x1="120" y1="120" x2="310" y2="120"/>
            <line x1="120" y1="150" x2="310" y2="150"/>
            <line x1="120" y1="150" x2="310" y2="120"/>
            <line x1="120" y1="180" x2="310" y2="150"/>
            <line x1="120" y1="180" x2="310" y2="180"/>
            <line x1="120" y1="210" x2="310" y2="180"/>
            <line x1="120" y1="210" x2="310" y2="210"/>
          </g>
          <g stroke="#ddd8ce" stroke-width="0.8" opacity="0.8">
            <line x1="370" y1="60"  x2="560" y2="85"/>
            <line x1="370" y1="60"  x2="560" y2="110"/>
            <line x1="370" y1="90"  x2="560" y2="85"/>
            <line x1="370" y1="90"  x2="560" y2="135"/>
            <line x1="370" y1="120" x2="560" y2="110"/>
            <line x1="370" y1="120" x2="560" y2="160"/>
            <line x1="370" y1="150" x2="560" y2="135"/>
            <line x1="370" y1="150" x2="560" y2="185"/>
            <line x1="370" y1="180" x2="560" y2="160"/>
            <line x1="370" y1="180" x2="560" y2="210"/>
            <line x1="370" y1="210" x2="560" y2="185"/>
            <line x1="370" y1="210" x2="560" y2="210"/>
          </g>
          <g fill="#1a1814" stroke="none">
            <circle cx="90" cy="60"  r="8"/>
            <circle cx="90" cy="90"  r="8"/>
            <circle cx="90" cy="120" r="8"/>
            <circle cx="90" cy="150" r="8"/>
            <circle cx="90" cy="180" r="8"/>
            <circle cx="90" cy="210" r="8"/>
            <text x="90" y="230" text-anchor="middle" font-size="18" fill="#7a746a">â‹®</text>
          </g>
          <g fill="none" stroke="#c4622d" stroke-width="2">
            <circle cx="340" cy="60"  r="10"/>
            <circle cx="340" cy="90"  r="10"/>
            <circle cx="340" cy="120" r="10"/>
            <circle cx="340" cy="150" r="10"/>
            <circle cx="340" cy="180" r="10"/>
            <circle cx="340" cy="210" r="10"/>
          </g>
          <g fill="#c4622d">
            <circle cx="340" cy="60"  r="5"/>
            <circle cx="340" cy="90"  r="5"/>
            <circle cx="340" cy="120" r="5"/>
            <circle cx="340" cy="150" r="5"/>
            <circle cx="340" cy="180" r="5"/>
            <circle cx="340" cy="210" r="5"/>
          </g>
          <g fill="none" stroke="#1d5fa8" stroke-width="2">
            <circle cx="590" cy="85"  r="13"/>
            <circle cx="590" cy="110" r="13"/>
            <circle cx="590" cy="135" r="13"/>
            <circle cx="590" cy="160" r="13"/>
            <circle cx="590" cy="185" r="13"/>
            <circle cx="590" cy="210" r="13"/>
          </g>
          <g fill="#1d5fa8" font-size="11" text-anchor="middle">
            <text x="590" y="89"  fill="white">0</text>
            <text x="590" y="114" fill="white">1</text>
            <text x="590" y="139" fill="white">2</text>
            <text x="590" y="164" fill="white">3</text>
            <text x="590" y="189" fill="white">4</text>
            <text x="590" y="214" fill="white">5</text>
          </g>
          <rect x="185" y="128" width="50" height="22" rx="4" fill="white" stroke="#ddd8ce"/>
          <text x="210" y="143" text-anchor="middle" font-size="12" fill="#c4622d" font-style="italic">Wâ‚, bâ‚</text>
          <rect x="435" y="128" width="50" height="22" rx="4" fill="white" stroke="#ddd8ce"/>
          <text x="460" y="143" text-anchor="middle" font-size="12" fill="#1d5fa8" font-style="italic">Wâ‚‚, bâ‚‚</text>
          <text x="340" y="38" text-anchor="middle" font-size="10" fill="#2d7a4f">ReLU</text>
          <text x="590" y="65" text-anchor="middle" font-size="10" fill="#2d7a4f">Softmax</text>
        </svg>
      </div>
    </div>

    <p class="reveal">The weights <code>W1</code> and <code>W2</code> are the learnable parameters â€” these are the numbers the network adjusts during training. Let's initialize them:</p>

    <div class="code-block reveal">
      <div class="code-header">
        <div class="code-dots"><div class="code-dot dot-r"></div><div class="code-dot dot-y"></div><div class="code-dot dot-g"></div></div>
        <div class="code-label">initialization.py</div>
      </div>
      <pre><span class="kw">def</span> <span class="fn">initialize_weights</span>(input_size=<span class="num">784</span>, hidden_size=<span class="num">128</span>, output_size=<span class="num">10</span>):
    <span class="cm"># He Initialization: scale = sqrt(2 / fan_in)</span>
    <span class="cm"># Tuned specifically for ReLU activations.</span>
    <span class="cm"># Too large â†’ exploding gradients. Too small â†’ nothing learns.</span>
    W1 = np.random.randn(input_size,  hidden_size) * np.sqrt(<span class="num">2.0</span> / input_size)
    b1 = np.zeros((<span class="num">1</span>, hidden_size))

    W2 = np.random.randn(hidden_size, output_size) * np.sqrt(<span class="num">2.0</span> / hidden_size)
    b2 = np.zeros((<span class="num">1</span>, output_size))

    <span class="kw">return</span> W1, b1, W2, b2</pre>
    </div>

    <div class="callout reveal">
      <div class="callout-icon">ğŸ’¡</div>
      <div>
        <div class="callout-title">Why not just use random numbers?</div>
        <p>If weights are too large, activations grow exponentially through each layer and "explode." If they're too small, gradients shrink to zero and learning stalls â€” the "vanishing gradient" problem. He initialization targets the sweet spot for ReLU networks.</p>
      </div>
    </div>
  </section>

  <div class="divider"><span>Â§</span></div>

  <!-- â•â•â•â• ACTIVATIONS â•â•â•â• -->
  <section id="forward">
    <h2><span class="step-num">Step 03</span>Activation Functions</h2>

    <p class="reveal">Without activation functions, stacking layers is pointless â€” every combination of linear operations is still just a linear operation. Activations introduce the non-linearity that lets networks learn complex patterns.</p>

    <div class="activation-grid reveal">
      <div class="activation-card">
        <h4>ReLU</h4>
        <div class="ac-desc">hidden layers</div>
        <canvas id="canvas-relu" width="180" height="120"></canvas>
      </div>
      <div class="activation-card">
        <h4>Softmax</h4>
        <div class="ac-desc">output layer</div>
        <canvas id="canvas-softmax" width="180" height="120"></canvas>
      </div>
    </div>

    <div class="code-block reveal">
      <div class="code-header">
        <div class="code-dots"><div class="code-dot dot-r"></div><div class="code-dot dot-y"></div><div class="code-dot dot-g"></div></div>
        <div class="code-label">activations.py</div>
      </div>
      <pre><span class="kw">def</span> <span class="fn">relu</span>(z):
    <span class="str">"""max(0, x) â€” zero for negatives, linear for positives."""</span>
    <span class="kw">return</span> np.maximum(<span class="num">0</span>, z)

<span class="kw">def</span> <span class="fn">softmax</span>(z):
    <span class="str">"""Turns raw scores into probabilities that sum to 1."""</span>
    <span class="cm"># Subtract max for numerical stability (prevents overflow)</span>
    shifted = z - np.max(z, axis=<span class="num">1</span>, keepdims=<span class="op">True</span>)
    exp_z = np.exp(shifted)
    <span class="kw">return</span> exp_z / np.sum(exp_z, axis=<span class="num">1</span>, keepdims=<span class="op">True</span>)</pre>
    </div>

    <h2><span class="step-num">Step 04</span>The Forward Pass</h2>

    <p class="reveal">The forward pass is simply propagating data through the network layer by layer. For each layer, we do two things: a linear transformation (matrix multiply + bias), then apply an activation function.</p>

    <div class="formula-box reveal">
      <div class="formula-row">
        <span class="formula-label">Layer 1:</span>
        <span class="formula-val">Z1 = X Â· W1 + b1</span>
        <span class="formula-comment">â† linear</span>
      </div>
      <div class="formula-row">
        <span class="formula-label"></span>
        <span class="formula-val">A1 = ReLU(Z1)</span>
        <span class="formula-comment">â† non-linear</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">Layer 2:</span>
        <span class="formula-val">Z2 = A1 Â· W2 + b2</span>
        <span class="formula-comment">â† linear</span>
      </div>
      <div class="formula-row">
        <span class="formula-label"></span>
        <span class="formula-val">A2 = Softmax(Z2)</span>
        <span class="formula-comment">â† probabilities</span>
      </div>
    </div>

    <div class="code-block reveal">
      <div class="code-header">
        <div class="code-dots"><div class="code-dot dot-r"></div><div class="code-dot dot-y"></div><div class="code-dot dot-g"></div></div>
        <div class="code-label">forward_pass.py</div>
      </div>
      <pre><span class="kw">def</span> <span class="fn">forward_pass</span>(X, W1, b1, W2, b2):
    <span class="cm"># Hidden layer</span>
    Z1 = X @ W1 + b1     <span class="cm"># (batch, 784) Ã— (784, 128) = (batch, 128)</span>
    A1 = relu(Z1)         <span class="cm"># apply non-linearity</span>

    <span class="cm"># Output layer</span>
    Z2 = A1 @ W2 + b2    <span class="cm"># (batch, 128) Ã— (128, 10) = (batch, 10)</span>
    A2 = softmax(Z2)      <span class="cm"># convert to probabilities</span>

    <span class="cm"># Cache everything â€” backprop needs these values!</span>
    <span class="kw">return</span> Z1, A1, Z2, A2</pre>
    </div>

    <div class="callout reveal">
      <div class="callout-icon">ğŸ“¦</div>
      <div>
        <div class="callout-title">Why cache Z1, A1?</div>
        <p>Backpropagation needs to know the intermediate values from the forward pass to compute gradients. Think of it as saving your work â€” you'll need it on the way back.</p>
      </div>
    </div>
  </section>

  <div class="divider"><span>Â§</span></div>

  <!-- â•â•â•â• LOSS â•â•â•â• -->
  <section id="loss">
    <h2><span class="step-num">Step 05</span>Loss Function: Measuring How Wrong We Are</h2>

    <p class="reveal">After a forward pass, the network produces a probability distribution over 10 classes. The <strong>loss function</strong> measures the gap between that prediction and the ground truth. We want to minimize this gap during training.</p>

    <p class="reveal">We use <strong>cross-entropy loss</strong> â€” the standard choice for classification. It heavily penalizes confident wrong answers, which pushes the network to be not just correct, but <em>genuinely certain</em> when it's right.</p>

    <div class="formula-box reveal">
      <div class="formula-row">
        <span class="formula-label">formula:</span>
        <span class="formula-val">Loss = âˆ’ (1/m) Â· Î£ Î£  Y Â· log(Å¶)</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">intuition:</span>
        <span class="formula-val">penalizes low probability on the correct class</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">perfect pred:</span>
        <span class="formula-val">loss â‰ˆ 0    (log(1) = 0)</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">wrong pred:</span>
        <span class="formula-val">loss â†’ âˆ   (log(0) = -âˆ)</span>
      </div>
    </div>

    <div class="code-block reveal">
      <div class="code-header">
        <div class="code-dots"><div class="code-dot dot-r"></div><div class="code-dot dot-y"></div><div class="code-dot dot-g"></div></div>
        <div class="code-label">loss.py</div>
      </div>
      <pre><span class="kw">def</span> <span class="fn">cross_entropy_loss</span>(A2, Y):
    m = Y.shape[<span class="num">0</span>]   <span class="cm"># number of samples in this batch</span>
    <span class="cm"># Add 1e-8 to avoid log(0), which is mathematically undefined</span>
    <span class="kw">return</span> -np.sum(Y * np.log(A2 + <span class="num">1e-8</span>)) / m</pre>
    </div>
  </section>

  <div class="divider"><span>Â§</span></div>

  <!-- â•â•â•â• BACKPROP â•â•â•â• -->
  <section id="backprop">
    <h2><span class="step-num">Step 06</span>Backpropagation: Learning from Mistakes</h2>

    <p class="reveal">This is the part that sounds intimidating but becomes beautiful once you see it. Backpropagation is just the <strong>chain rule of calculus</strong> applied repeatedly from the output layer back to the input.</p>

    <p class="reveal">The goal: figure out how much each weight contributed to the loss. If a weight made the prediction worse, we reduce it. If it helped, we keep or increase it. The gradient tells us the direction and amount.</p>

    <blockquote class="reveal">
      <p>"Backpropagation is blame assignment. Given that our prediction was wrong, which weights are responsible, and by how much?"</p>
    </blockquote>

    <p class="reveal">Here's the remarkable thing about the softmax + cross-entropy combination: their combined derivative is elegant â€” it's simply the error at the output:</p>

    <div class="formula-box reveal">
      <div class="formula-row">
        <span class="formula-label">Output error:</span>
        <span class="formula-val">dZ2 = A2 âˆ’ Y</span>
        <span class="formula-comment">â† prediction minus truth</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">Weight grad:</span>
        <span class="formula-val">dW2 = A1áµ€ Â· dZ2 / m</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">Propagate:</span>
        <span class="formula-val">dA1 = dZ2 Â· W2áµ€</span>
        <span class="formula-comment">â† push error back</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">ReLU gate:</span>
        <span class="formula-val">dZ1 = dA1 Ã— ReLUâ€²(Z1)</span>
        <span class="formula-comment">â† zero out negatives</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">Weight grad:</span>
        <span class="formula-val">dW1 = Xáµ€ Â· dZ1 / m</span>
      </div>
    </div>

    <div class="code-block reveal">
      <div class="code-header">
        <div class="code-dots"><div class="code-dot dot-r"></div><div class="code-dot dot-y"></div><div class="code-dot dot-g"></div></div>
        <div class="code-label">backprop.py</div>
      </div>
      <pre><span class="kw">def</span> <span class="fn">backward_pass</span>(X, Y, Z1, A1, A2, W2):
    m = X.shape[<span class="num">0</span>]

    <span class="cm"># â”€â”€ Output layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
    <span class="cm"># The combined derivative of softmax + cross-entropy</span>
    <span class="cm"># simplifies beautifully to just prediction minus truth.</span>
    dZ2 = A2 - Y                                   <span class="cm"># error signal</span>
    dW2 = A1.T @ dZ2 / m                           <span class="cm"># how to adjust W2</span>
    db2 = np.mean(dZ2, axis=<span class="num">0</span>, keepdims=<span class="op">True</span>)      <span class="cm"># how to adjust b2</span>

    <span class="cm"># â”€â”€ Hidden layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
    <span class="cm"># Pass the error signal back through W2...</span>
    dA1 = dZ2 @ W2.T
    <span class="cm"># ...then gate it through the ReLU derivative (1 or 0)</span>
    dZ1 = dA1 * (Z1 > <span class="num">0</span>).astype(float)
    dW1 = X.T @ dZ1 / m                            <span class="cm"># how to adjust W1</span>
    db1 = np.mean(dZ1, axis=<span class="num">0</span>, keepdims=<span class="op">True</span>)      <span class="cm"># how to adjust b1</span>

    <span class="kw">return</span> dW1, db1, dW2, db2</pre>
    </div>

    <h3>Gradient Descent: Taking a Step Downhill</h3>

    <p class="reveal">Once we have the gradients, updating weights is straightforward. We take a small step in the direction that reduces the loss. The <strong>learning rate</strong> controls how big that step is.</p>

    <div class="code-block reveal">
      <div class="code-header">
        <div class="code-dots"><div class="code-dot dot-r"></div><div class="code-dot dot-y"></div><div class="code-dot dot-g"></div></div>
        <div class="code-label">gradient_descent.py</div>
      </div>
      <pre><span class="kw">def</span> <span class="fn">update_weights</span>(W1, b1, W2, b2, dW1, db1, dW2, db2, lr=<span class="num">0.1</span>):
    <span class="cm"># W = W - learning_rate * gradient</span>
    <span class="cm"># The minus sign: we go in the direction that REDUCES the loss</span>
    W1 -= lr * dW1
    b1 -= lr * db1
    W2 -= lr * dW2
    b2 -= lr * db2
    <span class="kw">return</span> W1, b1, W2, b2</pre>
    </div>

    <div class="callout reveal">
      <div class="callout-icon">âš ï¸</div>
      <div>
        <div class="callout-title">Learning Rate is the Most Sensitive Knob</div>
        <p>Too large (e.g. 10.0) and the loss will bounce chaotically or explode. Too small (e.g. 0.00001) and training will take forever. A good starting range is 0.01â€“0.1. Start at 0.1 and halve it if training diverges.</p>
      </div>
    </div>
  </section>

  <div class="divider"><span>Â§</span></div>

  <!-- â•â•â•â• TRAINING LOOP â•â•â•â• -->
  <section id="training">
    <h2><span class="step-num">Step 07</span>The Training Loop</h2>

    <p class="reveal">Now we put all the pieces together. Each iteration of the loop is called a <strong>mini-batch</strong> â€” we take a chunk of 256 training samples, run forward pass, compute loss, run backprop, update weights. When we've gone through the entire training set once, that's one <strong>epoch</strong>.</p>

    <p class="reveal">Mini-batches strike the balance between stochastic gradient descent (one sample at a time â€” noisy but fast) and full-batch gradient descent (all samples â€” stable but slow). 256 is a classic choice.</p>

    <div class="code-block reveal">
      <div class="code-header">
        <div class="code-dots"><div class="code-dot dot-r"></div><div class="code-dot dot-y"></div><div class="code-dot dot-g"></div></div>
        <div class="code-label">training_loop.py</div>
      </div>
      <pre><span class="kw">def</span> <span class="fn">train</span>(X_train, Y_train, X_val, Y_val,
          hidden_size=<span class="num">128</span>, lr=<span class="num">0.1</span>, epochs=<span class="num">20</span>, batch_size=<span class="num">256</span>):

    W1, b1, W2, b2 = initialize_weights(hidden_size=hidden_size)
    m = X_train.shape[<span class="num">0</span>]   <span class="cm"># 60,000</span>

    <span class="kw">for</span> epoch <span class="kw">in</span> range(<span class="num">1</span>, epochs + <span class="num">1</span>):

        <span class="cm"># â”€â”€ Shuffle data each epoch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
        <span class="cm"># Prevents the model from memorizing sample order</span>
        idx = np.random.permutation(m)
        X_shuf, Y_shuf = X_train[idx], Y_train[idx]

        <span class="cm"># â”€â”€ Mini-batch loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
        <span class="kw">for</span> start <span class="kw">in</span> range(<span class="num">0</span>, m, batch_size):
            Xb = X_shuf[start : start + batch_size]
            Yb = Y_shuf[start : start + batch_size]

            <span class="cm"># The four steps, every single batch:</span>
            Z1, A1, Z2, A2 = forward_pass(Xb, W1, b1, W2, b2)    <span class="cm"># 1. predict</span>
            loss = cross_entropy_loss(A2, Yb)                       <span class="cm"># 2. measure</span>
            dW1, db1, dW2, db2 = backward_pass(Xb, Yb, Z1, A1, A2, W2) <span class="cm"># 3. blame</span>
            W1, b1, W2, b2 = update_weights(W1, b1, W2, b2, ...)   <span class="cm"># 4. improve</span>

        <span class="cm"># Print progress once per epoch</span>
        _, _, _, A2_val = forward_pass(X_val, W1, b1, W2, b2)
        val_acc = np.mean(np.argmax(A2_val, axis=<span class="num">1</span>) == np.argmax(Y_val, axis=<span class="num">1</span>))
        print(f<span class="str">"Epoch {epoch:>2}  |  loss: {loss:.4f}  |  val_acc: {val_acc:.2%}"</span>)

    <span class="kw">return</span> W1, b1, W2, b2</pre>
    </div>
  </section>

  <div class="divider"><span>Â§</span></div>

  <!-- â•â•â•â• RESULTS â•â•â•â• -->
  <section id="results">
    <h2><span class="step-num">Step 08</span>Results & What Happens as it Trains</h2>

    <p class="reveal">Here's what the output looks like as the model trains. Watch both the loss (going down is good) and the validation accuracy (going up is good):</p>

    <table class="results-table reveal">
      <thead>
        <tr>
          <th>Epoch</th>
          <th>Train Loss</th>
          <th>Val Accuracy</th>
          <th>Notes</th>
        </tr>
      </thead>
      <tbody>
        <tr><td>1</td>  <td>0.4821</td><td>88.3%</td><td>Already better than random</td></tr>
        <tr><td>2</td>  <td>0.2634</td><td>92.8%</td><td>Fast initial learning</td></tr>
        <tr><td>5</td>  <td>0.1573</td><td>95.4%</td><td>Slowing down</td></tr>
        <tr><td>10</td> <td>0.1102</td><td>96.7%</td><td>Convergence region</td></tr>
        <tr><td>15</td> <td>0.0871</td><td>97.2%</td><td>Diminishing returns</td></tr>
        <tr><td>20</td> <td>0.0732</td><td>97.6%</td><td>Final result âœ“</td></tr>
      </tbody>
    </table>

    <p class="reveal"><strong>97.6% accuracy.</strong> That means out of 10,000 test digits the network has never seen, it correctly identifies all but ~240. That's remarkable for a single hidden layer and pure NumPy.</p>

    <h3>Experiment Suggestions</h3>

    <p class="reveal">The real learning happens when you start poking at the knobs. Here's what to try and what you'll observe:</p>

    <div class="formula-box reveal" style="line-height:2.2;">
      <div class="formula-row">
        <span class="formula-label">hidden_size=64</span>
        <span class="formula-eq">â†’</span>
        <span class="formula-val">faster, ~96%</span>
        <span class="formula-comment">less capacity</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">hidden_size=512</span>
        <span class="formula-eq">â†’</span>
        <span class="formula-val">~98%, slower</span>
        <span class="formula-comment">more capacity, risk overfit</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">lr=0.5</span>
        <span class="formula-eq">â†’</span>
        <span class="formula-val">fast early</span>
        <span class="formula-comment">may oscillate or diverge</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">lr=0.001</span>
        <span class="formula-eq">â†’</span>
        <span class="formula-val">very stable</span>
        <span class="formula-comment">needs more epochs</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">batch_size=32</span>
        <span class="formula-eq">â†’</span>
        <span class="formula-val">noisier updates</span>
        <span class="formula-comment">can generalize better</span>
      </div>
      <div class="formula-row">
        <span class="formula-label">2 hidden layers</span>
        <span class="formula-eq">â†’</span>
        <span class="formula-val">deeper network</span>
        <span class="formula-comment">needs more backprop code</span>
      </div>
    </div>

    <h3>What's Next?</h3>

    <p class="reveal">You now understand everything a basic neural network does. From here, every modern deep learning concept is just an extension of what you've built:</p>

    <div class="callout reveal">
      <div class="callout-icon">ğŸš€</div>
      <div>
        <div class="callout-title">Natural Extensions to Explore</div>
        <p><strong>Adam optimizer</strong> â€” adaptive learning rates per parameter. <strong>Dropout</strong> â€” randomly zeroing neurons during training to prevent overfitting. <strong>Batch normalization</strong> â€” normalizing activations within a layer. <strong>Convolutional layers</strong> â€” designed for images, exploiting spatial structure. Once you understand the math here, all of these are just new pieces plugged into the same loop.</p>
      </div>
    </div>

    <blockquote class="reveal">
      <p>You didn't install a framework. You didn't call <code>model.fit()</code>. You built the whole thing yourself â€” and it reads handwriting at 97% accuracy. That's not beginner work. That's engineering.</p>
    </blockquote>
  </section>

</article>
</div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• FOOTER â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<footer>
  <p style="font-size:14px;margin-bottom:8px;">Built with NumPy, patience, and a love of math.</p>
  <p>Full code available in <code style="background:#2a2720;border-color:#3a3730;color:#98c379;">mnist_from_scratch.py</code></p>
</footer>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• SCRIPTS â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<script src="../js/main.js"></script>
<script>
// â”€â”€ Progress bar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
window.addEventListener('scroll', () => {
  const doc  = document.documentElement;
  const top  = doc.scrollTop  || document.body.scrollTop;
  const h    = doc.scrollHeight - doc.clientHeight;
  document.getElementById('progress-bar').style.width = (top / h * 100) + '%';
});

// â”€â”€ Scroll reveal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const observer = new IntersectionObserver(entries => {
  entries.forEach(e => { if (e.isIntersecting) { e.target.classList.add('visible'); } });
}, { threshold: 0.1 });
document.querySelectorAll('.reveal').forEach(el => observer.observe(el));

// â”€â”€ Pixel grid visual (stylized digit "3") â”€â”€â”€â”€â”€â”€â”€â”€
const digit3 = [
  0,0,0,0,0,0,0,0,0,0,0,0,0,0,
  0,0,0,1,2,3,3,3,2,1,0,0,0,0,
  0,0,2,3,3,2,2,3,3,3,1,0,0,0,
  0,0,3,3,1,0,0,0,2,3,3,0,0,0,
  0,0,1,2,0,0,0,0,2,3,2,0,0,0,
  0,0,0,0,0,0,1,2,3,3,0,0,0,0,
  0,0,0,0,0,2,3,3,3,1,0,0,0,0,
  0,0,0,0,0,0,1,2,3,3,0,0,0,0,
  0,0,1,1,0,0,0,0,2,3,2,0,0,0,
  0,0,3,3,1,0,0,0,1,3,3,0,0,0,
  0,0,2,3,3,2,1,2,3,3,1,0,0,0,
  0,0,0,1,2,3,3,3,2,1,0,0,0,0,
  0,0,0,0,0,0,0,0,0,0,0,0,0,0,
  0,0,0,0,0,0,0,0,0,0,0,0,0,0
];

const grid = document.getElementById('pixel-grid-display');
const colors = ['#faf8f3','#ddd8ce','#aaa49a','#5a5650','#1a1814'];
digit3.forEach(v => {
  const px = document.createElement('div');
  px.className = 'pixel';
  px.style.background = colors[v];
  grid.appendChild(px);
});

// â”€â”€ Output probabilities visual â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const probs = [
  {d:0, p:0.00}, {d:1, p:0.00}, {d:2, p:0.01}, {d:3, p:0.96},
  {d:4, p:0.00}, {d:5, p:0.01}, {d:6, p:0.00}, {d:7, p:0.01},
  {d:8, p:0.01}, {d:9, p:0.00}
];
const probsContainer = document.getElementById('probs-display');
probs.forEach(({d, p}) => {
  const row = document.createElement('div');
  row.className = 'prob-row';
  const isTop = d === 3;
  row.innerHTML = `
    <span class="prob-digit">${d}</span>
    <div class="prob-bar-wrap">
      <div class="prob-bar ${isTop ? 'top' : ''}" style="width:${Math.round(p*100)}%"></div>
    </div>
    <span class="prob-val">${(p*100).toFixed(0)}%</span>
  `;
  probsContainer.appendChild(row);
});

// â”€â”€ Activation function charts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function drawActivation(canvasId, fn, color, label) {
  const canvas = document.getElementById(canvasId);
  if (!canvas) return;
  const ctx = canvas.getContext('2d');
  const W = canvas.width, H = canvas.height;
  const pad = 16;
  const xRange = 6;

  ctx.clearRect(0, 0, W, H);
  ctx.fillStyle = '#ffffff';
  ctx.fillRect(0, 0, W, H);

  ctx.strokeStyle = '#ddd8ce';
  ctx.lineWidth = 1;
  ctx.beginPath();
  ctx.moveTo(pad, H/2); ctx.lineTo(W-pad, H/2);
  ctx.moveTo(W/2, pad); ctx.lineTo(W/2, H-pad);
  ctx.stroke();

  ctx.fillStyle = '#7a746a';
  ctx.font = '10px JetBrains Mono, monospace';
  ctx.textAlign = 'center';
  ctx.fillText('0', W/2 + 6, H/2 - 4);

  ctx.strokeStyle = color;
  ctx.lineWidth = 2.5;
  ctx.beginPath();
  for (let i = 0; i <= W - 2*pad; i++) {
    const x = (i / (W - 2*pad)) * 2 * xRange - xRange;
    const y = fn(x);
    const px = pad + i;
    const py = H/2 - (y * (H/2 - pad) / (fn.max || 1));
    i === 0 ? ctx.moveTo(px, py) : ctx.lineTo(px, py);
  }
  ctx.stroke();
}

const reluFn = x => Math.max(0, x);
reluFn.max = 6;
drawActivation('canvas-relu', reluFn, '#c4622d');

const softmaxFn = x => 1 / (1 + Math.exp(-x));
softmaxFn.max = 1;
drawActivation('canvas-softmax', softmaxFn, '#1d5fa8');
</script>
</body>
</html>
